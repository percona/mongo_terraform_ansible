# Deploy MongoDB clusters using Ansible
## Quick guide
1. Prepare an `inventory` file (or use one generated by Terraform)
2. Edit the [variable files](group_vars/l) files to choose the options, passwords, ports, etc.
3. Run the `main.yml` playbook pointing Ansible to the desired inventory file. For example:
```
cd ./ansible
ansible-playbook main.yml -i inventory
```

The installation process takes around 15 min for a 2 shard cluster.
To connect to the servers there is a helper script provisioned in /etc/profile. you can run directly `mongo` on any host to connect to it locally with the proper credentials.

## Inventory file

- Keep a separate inventory file per environment (for example dev, test, prod).
- If you have more than one cluster per environment, then keep one inventory per cluster as well (for example devrs1, devrs2, devshard1, devshard2).
- On each inventory file, we have to specify:
  - a group for each shard's replicaset (for example `shardXXXX`). Set all these groups as children of `shards`.
  - a group for the config servers (name it `cfg`)
  - a group for mongos routers (name it `mongos`).
- You can specify a server in more than one group only for the case of deploying a mongos + config server on the same host. Any other combinations are currently not supported and cause execution to fail.
- Arbiters are specified by adding `arbiter=True` tag to each arbiter host

- Example of an inventory for a sharded cluster (GCP):
```
[cfg]
dev-mongo-cfg00 mongodb_primary=True
dev-mongo-cfg01
dev-mongo-cfg02

[shards:children]
shard0
shard1

[shard0]
dev-mongo-shard00svr0 mongodb_primary=True
dev-mongo-shard00svr1
dev-mongo-shard00svr2

[shard1]
dev-mongo-shard01svr0 mongodb_primary=True
dev-mongo-shard01svr1
dev-mongo-shard01svr2 arbiter=True

[mongos]
dev-mongo-router00

[pmm]
dev-percona-pmm

[all:vars]
ansible_ssh_user=mongodb
pmm_public_ip=30.30.30.30
pmm_private_ip=192.168.0.1
bucket=dev-mongo-backups
region=NORTHAMERICA-NORTHEAST1
cluster=dev
access_key_id=GOOG1E2TPIJ5*****
secret_access_key=******
```

- For each standalone replicaset you need to create a group and put all them as children of `replset` group:
```
[replset:children]
rs1
rs2

[rs1]
host11 mongodb_primary=True
host12
host13

[rs2]
host14 mongodb_primary=True
host15
host16

[pmm]
test-percona-pmm

[all:vars]
ansible_ssh_user=mongodb
pmm_public_ip=30.30.30.30
pmm_private_ip=192.168.0.1
bucket=test-mongo-backups
region=NORTHAMERICA-NORTHEAST1
cluster=test
access_key_id=GOOG1E2TPI*******
secret_access_key=******
```

The `mongodb_primary` tag will make that server become the primary by giving it higher priority in the replicaset configuration.
Only 1 sharded cluster per inventory file is supported at this time. If you need more then use separate inventory files (See above instructions).

## Configuration
* The [all variables file](group_vars/all.yml) contains all the user-modifiable parameters. Each of these come with a small description to clarify the purpose, unless it is self-explanatory.
* The rest of the variable files contain per-role variables you can also modify.
You should review and modify these files as needed before making the deployment.

Note: The PMM user and password are used to login to PMM UI via the web browser. If you want to change the default PMM credentials (admin/admin), you will need to login to PMM via the web browser and change the PMM password there after deployment finishes.

## Running
* The playbook is meant to handle a deployment from scratch, unless run with some specific tags. Be extra careful if you are running it against servers that already have data.

* Deploy a replica set or sharded cluster from scratch:
```
ansible-playbook main.yml -i inventory
```
* Add a new shard (e.g. shard3) to an existing cluster:
```
ansible-playbook main.yml -i inventory --limit shard3
```
* Deploy skip the monitoring and backup parts
```
ansible-playbook main.yml -i inventory --skip-tags monitoring,backup
```

## TLS Setup

There is an extra playbook `tls-setup.yml` that generates a test CA on the PMM server, then proceeds to create server certificates for each host. Finally certificates are copied to the configured location on each host. Run this playbook before the `main.yml` if you want a TLS-enabled setup and you don't have any certificates prepared in advance. Remember to set `use_tls: true` in the variables file in advance.

```
ansible-playbook tls-setup.yml -i inventory
```

### Available tags for `main.yml`:
  - os_conf
    - Tunes the OS for MongoDB
  - install
    - Installs packages at the OS level
  - mongo_cfgsrv
    - Configures the config servers
  - mongo_rs
    - Configures the replica sets
  - bootstrap_rs
    - Bootstraps the replica sets (rs.initiate())
  - mongos
    - Configures the mongos routers
  - add_shards
    - Runs the sh.addShard() command on a mongos router
  - backup
    - Deploys & configures the pbm agent    
  - pmm_server
    - Deploys a PMM server with docker
  - monitoring
    - Deploys PMM client and registers with a pmm server

### Available tags for `tls-setup.yml`:
  - ca
    - Deploys the CA on PMM-server
  - create_certs
    - Creates the certificates for each server and client
  - copy_certs
    - Copies certificates from control machine to each server

## Cleanup
* If you want cleanup a failed deploy to retry, usually stopping mongod/mongos components and removing the datadir content is enough e.g.
```
service mongos stop; service mongod stop; rm -rf /var/lib/mongo/*
```

You can also use the `reset.yml` playbook to do this for you:
```
ansible-playbook reset.yml -i inventory
```

## Stopping
* To stop all mongod, mongos, pmm and pbm-agent services you can run the `stop.yml` playbook:
```
ansible-playbook stop.yml -i inventory
```

## Restarting
* To restart all mongod, mongos, pmm and pbm-agent services you can run the `restart.yml` playbook:
```
ansible-playbook restart.yml -i inventory
```

This is useful for example after restoring a physical backup. The playbook includes a resync of PBM backup inventory at the end.

## Connecting
* Connection string example (no TLS)
```
mongo admin -u root -p percona --port 27017
```

* Connection string example with TLS
```
mongo admin --tls --tlsCAFile /etc/ssl/test-ca.pem --tlsCertificateKeyFile /etc/ssl/client.pem --port 27017 --host ip-10-0-1-199.ec2.internal -u root -p percona
```

* List existing backups with TLS
```
sudo -i
pbm list --mongodb-uri "mongodb://pbm:secretpwd@ip-10-0-1-199.ec2.internal:27019/?tls=true&tlsCertificateKeyFile=/etc/ssl/server.pem&tlsCAFile=/etc/ssl/test-ca.pem"
```

## Adding components to an existing deployment
* You can add extra mongos routers by including them on the inventory file, then running the playbook using the appropriate tags and limit.
```
ansible-playbook main.yml -i inventory --tags mongos,monitoring --limit mongos
```
